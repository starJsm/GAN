{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "# import pytorch_lightning as pl\n",
    "from pytorch_lightning import  LightningModule, Trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sys\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    # python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 为了结果可复现\n",
    "Seed = 42\n",
    "same_seeds(Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    " # shuffle 是否将官方给的的测试集和训练集重新打乱，再分成新的的训练集和测试集\n",
    " # ss标准化\n",
    "def process_data(tr_data, te_data=None, ss=None, shuffle=False):\n",
    "    split_num = len(tr_data)\n",
    "    data_temp = pd.concat([tr_data, te_data], axis=0)\n",
    "    data = pd.get_dummies(data_temp.iloc[:, 1:-2])\n",
    "    data['cat_code'] = LabelEncoder().fit_transform(data_temp.loc[:, 'attack_cat'])\n",
    "    # data['label'] = data_temp['label']\n",
    "    # data['attack_cat'] = data_temp['attack_cat']\n",
    "    if ss != None:\n",
    "        data.iloc[:,:-3] = ss.fit_transform(data.iloc[:,:-3])\n",
    "    if shuffle:\n",
    "        pass\n",
    "    else:\n",
    "        return data.iloc[:split_num,:], data.iloc[split_num:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dur</th>\n",
       "      <th>spkts</th>\n",
       "      <th>dpkts</th>\n",
       "      <th>sbytes</th>\n",
       "      <th>dbytes</th>\n",
       "      <th>rate</th>\n",
       "      <th>sttl</th>\n",
       "      <th>dttl</th>\n",
       "      <th>sload</th>\n",
       "      <th>dload</th>\n",
       "      <th>...</th>\n",
       "      <th>service_ssl</th>\n",
       "      <th>state_ACC</th>\n",
       "      <th>state_CLO</th>\n",
       "      <th>state_CON</th>\n",
       "      <th>state_ECO</th>\n",
       "      <th>state_FIN</th>\n",
       "      <th>state_INT</th>\n",
       "      <th>state_PAR</th>\n",
       "      <th>state_REQ</th>\n",
       "      <th>state_RST</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.188346</td>\n",
       "      <td>-0.101342</td>\n",
       "      <td>-0.129612</td>\n",
       "      <td>-0.047849</td>\n",
       "      <td>-0.097232</td>\n",
       "      <td>-0.568650</td>\n",
       "      <td>0.702512</td>\n",
       "      <td>1.500906</td>\n",
       "      <td>-0.380090</td>\n",
       "      <td>-0.269328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.00394</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.291137</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>1.095103</td>\n",
       "      <td>-0.90798</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>-0.018058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.099897</td>\n",
       "      <td>-0.042496</td>\n",
       "      <td>0.173998</td>\n",
       "      <td>-0.045110</td>\n",
       "      <td>0.188966</td>\n",
       "      <td>-0.568623</td>\n",
       "      <td>-1.151363</td>\n",
       "      <td>1.483170</td>\n",
       "      <td>-0.380121</td>\n",
       "      <td>-0.064104</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.00394</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.291137</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>1.095103</td>\n",
       "      <td>-0.90798</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>-0.018058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.063006</td>\n",
       "      <td>-0.086630</td>\n",
       "      <td>-0.022456</td>\n",
       "      <td>-0.047239</td>\n",
       "      <td>-0.008217</td>\n",
       "      <td>-0.569024</td>\n",
       "      <td>-1.151363</td>\n",
       "      <td>1.483170</td>\n",
       "      <td>-0.380158</td>\n",
       "      <td>-0.247593</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.00394</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.291137</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>1.095103</td>\n",
       "      <td>-0.90798</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>-0.018058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.072800</td>\n",
       "      <td>-0.057207</td>\n",
       "      <td>-0.058174</td>\n",
       "      <td>-0.045720</td>\n",
       "      <td>-0.093142</td>\n",
       "      <td>-0.569027</td>\n",
       "      <td>-1.151363</td>\n",
       "      <td>1.483170</td>\n",
       "      <td>-0.380152</td>\n",
       "      <td>-0.271458</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.00394</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.291137</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>1.095103</td>\n",
       "      <td>-0.90798</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>-0.018058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.133449</td>\n",
       "      <td>-0.071919</td>\n",
       "      <td>-0.111753</td>\n",
       "      <td>-0.046261</td>\n",
       "      <td>-0.096576</td>\n",
       "      <td>-0.568904</td>\n",
       "      <td>0.722026</td>\n",
       "      <td>1.483170</td>\n",
       "      <td>-0.380121</td>\n",
       "      <td>-0.271197</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018272</td>\n",
       "      <td>-0.00394</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.291137</td>\n",
       "      <td>-0.006824</td>\n",
       "      <td>1.095103</td>\n",
       "      <td>-0.90798</td>\n",
       "      <td>-0.00197</td>\n",
       "      <td>-0.122882</td>\n",
       "      <td>-0.018058</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 194 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        dur     spkts     dpkts    sbytes    dbytes      rate      sttl  \\\n",
       "0 -0.188346 -0.101342 -0.129612 -0.047849 -0.097232 -0.568650  0.702512   \n",
       "1 -0.099897 -0.042496  0.173998 -0.045110  0.188966 -0.568623 -1.151363   \n",
       "2  0.063006 -0.086630 -0.022456 -0.047239 -0.008217 -0.569024 -1.151363   \n",
       "3  0.072800 -0.057207 -0.058174 -0.045720 -0.093142 -0.569027 -1.151363   \n",
       "4 -0.133449 -0.071919 -0.111753 -0.046261 -0.096576 -0.568904  0.722026   \n",
       "\n",
       "       dttl     sload     dload  ...  service_ssl  state_ACC  state_CLO  \\\n",
       "0  1.500906 -0.380090 -0.269328  ...    -0.018272   -0.00394   -0.00197   \n",
       "1  1.483170 -0.380121 -0.064104  ...    -0.018272   -0.00394   -0.00197   \n",
       "2  1.483170 -0.380158 -0.247593  ...    -0.018272   -0.00394   -0.00197   \n",
       "3  1.483170 -0.380152 -0.271458  ...    -0.018272   -0.00394   -0.00197   \n",
       "4  1.483170 -0.380121 -0.271197  ...    -0.018272   -0.00394   -0.00197   \n",
       "\n",
       "   state_CON  state_ECO  state_FIN  state_INT  state_PAR  state_REQ  state_RST  \n",
       "0  -0.291137  -0.006824   1.095103   -0.90798   -0.00197  -0.122882  -0.018058  \n",
       "1  -0.291137  -0.006824   1.095103   -0.90798   -0.00197  -0.122882  -0.018058  \n",
       "2  -0.291137  -0.006824   1.095103   -0.90798   -0.00197  -0.122882  -0.018058  \n",
       "3  -0.291137  -0.006824   1.095103   -0.90798   -0.00197  -0.122882  -0.018058  \n",
       "4  -0.291137  -0.006824   1.095103   -0.90798   -0.00197  -0.122882  -0.018058  \n",
       "\n",
       "[5 rows x 194 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 加载数据未处理的数据\n",
    "tr_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_testing-set.csv')\n",
    "te_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_training-set.csv')\n",
    "ss = StandardScaler()\n",
    "# 调用数据处理函数\n",
    "tr_data, te_data = process_data(tr_raw_data, te_raw_data, ss)\n",
    "# 挑选'Normal'的列，'cat_code'=6\n",
    "tr_data = tr_data.loc[tr_data['cat_code'] == 6]\n",
    "tr_data.drop(['cat_code'], axis=1, inplace=True)\n",
    "# 去掉无用的列\n",
    "tr_data.drop(['state_URN', 'state_no'], axis=1, inplace=True)\n",
    "tr_data.head()\n",
    "\n",
    "# raw_data = pd.read_csv('/home/jsm/code/python_backup/python/IoT-botnet/data/UNSW-NB15 - CSV Files/unsw15_train.csv')\n",
    "# temp = raw_data.loc[raw_data['attack_cat'] == 'Normal']\n",
    "# temp_drop = temp.drop(['196', 'attack_cat', 'label'], axis=1, inplace=False)\n",
    "# temp_sameple = temp_drop.sample(1024*60, random_state=Seed)\n",
    "# tr_data = temp_sameple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # batch_size,\n",
    "        # num_workers,\n",
    "        data\n",
    "    ):\n",
    "        # 在数据1维处增加1个维度 example: (batch_size, 196) --> (batch_size, 1, 196)\n",
    "        # self.batch_size = batch_size\n",
    "        # self.num_workers = num_workers\n",
    "        self.data = data.unsqueeze(1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = MyDataset(torch.from_numpy(tr_data.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络参数初始化\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # 初始化网络层\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, in_dim)\n",
    "    Output shape: (N, 1, out_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose1d(in_dim, out_dim, 5, 2, padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm1d(out_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        self.inlayer = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim*4*4*4, bias=False),\n",
    "            # tf 默认为0.3， torch 默认为0.01\n",
    "            nn.BatchNorm1d(dim*4*4*4),\n",
    "            nn.ReLU()\n",
    "            # nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.midlayer = nn.Sequential(\n",
    "           dconv_bn_relu(dim*4, dim*2),\n",
    "        #    dconv_bn_relu(dim*2, dim*2),\n",
    "           dconv_bn_relu(dim*2, dim),\n",
    "           dconv_bn_relu(dim, 1)\n",
    "        )\n",
    "        self.outlayer = nn.Sequential(\n",
    "            nn.Linear(128, out_dim, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.inlayer(x)\n",
    "        y = y.view(y.size(0), -1, 16)\n",
    "        y = self.midlayer(y)\n",
    "        y = y.squeeze(1)\n",
    "        y = self.outlayer(y)\n",
    "        y = y.unsqueeze(1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判别器\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 1, in_dim)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=256, in_channel=1, channel=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def conv_bn_lrelu(in_channel, out_channel):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channel, out_channel, 5, 2, 2),\n",
    "                nn.BatchNorm1d(out_channel),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "\n",
    "        self.inlayer = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.ls = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, channel, 5, 2, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv_bn_lrelu(channel, channel * 2),\n",
    "            conv_bn_lrelu(channel * 2, channel * 4),\n",
    "            conv_bn_lrelu(channel * 4, channel * 8),\n",
    "            conv_bn_lrelu(channel * 8, channel * 16),\n",
    "            conv_bn_lrelu(channel * 16, channel * 32),\n",
    "            nn.Conv1d(channel * 32, 1, 4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x.squeeze(1)\n",
    "        y = self.inlayer(y)\n",
    "        y = y.unsqueeze(1)\n",
    "        y = self.ls(y)\n",
    "        y = y.view(-1)\n",
    "        return y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 16,\n",
    "        out_dim: int = 128,\n",
    "        in_channels: int = 1,\n",
    "        lr: float = 1e-4,\n",
    "        n_critic: int = 5,\n",
    "        clip_value: float = 0.01,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        self.generator = Generator(in_dim=self.hparams.in_dim, out_dim=self.hparams.out_dim)\n",
    "        self.discriminator = Discriminator(in_dim = self.hparams.out_dim)\n",
    "        # self.validation_z = torch.randn(10, self.hparams.in_channels, self.hparams.in_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return -torch.mean(self.discriminator(y)) + torch.mean(self.discriminator(y_hat))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "\n",
    "        data = batch\n",
    "        z = torch.randn(self.hparams.batch_size, self.hparams.in_dim)\n",
    "        z = z.type_as(data)\n",
    "        # train generator\n",
    "        if optimizer_idx == 0 and (batch_idx % self.hparams.n_critic == 0 and batch_idx != 0):\n",
    "            # print('batch_idx {}, optimizer_idx{}'.format(batch_idx, optimizer_idx))\n",
    "            # generate data\n",
    "            self.generated_data = self(z)\n",
    "\n",
    "            # generator of WGAN loss\n",
    "            g_loss = -torch.mean(self.discriminator(self(z)))\n",
    "            self.logger.experiment.add_scalar(\"g_loss\", g_loss.detach(), self.current_epoch)\n",
    "            tqdm_dict = {\"g_loss\": g_loss.detach()}\n",
    "            output = OrderedDict({\"loss\": g_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            # print('batch_idx {}, optimizer_idx{}'.format(batch_idx, optimizer_idx))\n",
    "            # discriminator of WGAN loss\n",
    "            d_loss = -torch.mean(self.discriminator(data)) + torch.mean(self.discriminator(self(z)))\n",
    "            self.logger.experiment.add_scalar(\"d_loss\", d_loss.detach(), self.current_epoch)\n",
    "            # Clip weights of discriminator\n",
    "            for p in self.discriminator.parameters():\n",
    "                p.data.clamp_(-self.hparams.clip_value, self.hparams.clip_value)\n",
    "\n",
    "            tqdm_dict = {\"d_loss\": d_loss.detach()}\n",
    "            output = OrderedDict({\"loss\": d_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n",
    "            return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        opt_g = torch.optim.RMSprop(self.generator.parameters(), lr=lr)\n",
    "        opt_d = torch.optim.RMSprop(self.discriminator.parameters(), lr=lr)\n",
    "        \n",
    "        return [opt_g, opt_d], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataloder = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "# in_dim = tr_data.shape[-1]\n",
    "in_dim = 16\n",
    "out_dim = tr_data.shape[-1]\n",
    "wgan = WGAN(in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 113 K \n",
      "1 | discriminator | Discriminator | 270 K \n",
      "------------------------------------------------\n",
      "384 K     Trainable params\n",
      "0         Non-trainable params\n",
      "384 K     Total params\n",
      "1.537     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  84%|████████▎ | 183/219 [00:02<00:00, 62.28it/s, loss=-0.000474, v_num=1]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsm/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/trainer.py:685: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3:  84%|████████▎ | 183/219 [00:13<00:02, 13.65it/s, loss=-0.000474, v_num=1]"
     ]
    }
   ],
   "source": [
    "# early_stop_callback = EarlyStopping(monitor=\"avg_f1\", min_delta=0.00, patience=10, verbose=False, mode=\"max\")\n",
    "trainer = Trainer(\n",
    "    gpus = AVAIL_GPUS,\n",
    "    max_epochs=10,\n",
    "    # progress_bar_refresh_rate = 20\n",
    ")\n",
    "trainer.fit(wgan, tr_dataloder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de5b82254768225213474ab4669cea3d52fe6b864ad6c9d79489ae1089fd4498"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
