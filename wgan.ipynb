{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "# import pytorch_lightning as pl\n",
    "from pytorch_lightning import  LightningModule, Trainer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#sys\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "import math\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seeds(seed):\n",
    "    # python random\n",
    "    random.seed(seed)\n",
    "    # Numpy\n",
    "    np.random.seed(seed)\n",
    "    # Torch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# 为了结果可复现\n",
    "Seed = 42\n",
    "same_seeds(Seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " # shuffle 是否将官方给的的测试集和训练集重新打乱，再分成新的的训练集和测试集\n",
    " # ss标准化\n",
    "def process_data(tr_data, te_data=None, ss=None, shuffle=False):\n",
    "    split_num = len(tr_data)\n",
    "    data_temp = pd.concat([tr_data, te_data], axis=0)\n",
    "    data = pd.get_dummies(data_temp.iloc[:, 1:-2])\n",
    "    data['cat_code'] = LabelEncoder().fit_transform(data_temp.loc[:, 'attack_cat'])\n",
    "    # data['label'] = data_temp['label']\n",
    "    # data['attack_cat'] = data_temp['attack_cat']\n",
    "    if ss != None:\n",
    "        data.iloc[:,:-3] = ss.fit_transform(data.iloc[:,:-3])\n",
    "    if shuffle:\n",
    "        pass\n",
    "    else:\n",
    "        return data.iloc[:split_num,:], data.iloc[split_num:, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 256 if AVAIL_GPUS else 64\n",
    "NUM_WORKERS = int(os.cpu_count() / 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载数据未处理的数据\n",
    "tr_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_testing-set.csv')\n",
    "te_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_training-set.csv')\n",
    "ss = StandardScaler()\n",
    "# 调用数据处理函数\n",
    "tr_data, te_data = process_data(tr_raw_data, te_raw_data, ss)\n",
    "# 挑选'Normal'的列，'cat_code'=6\n",
    "tr_data = tr_data.loc[tr_data['cat_code'] == 6]\n",
    "tr_data.drop(['cat_code'], axis=1, inplace=True)\n",
    "# 去掉无用的列\n",
    "# tr_data.drop(['state_URN', 'state_no', 'cat_code'], axis=1, inplace=True)\n",
    "tr_data.head()\n",
    "\n",
    "# raw_data = pd.read_csv('/home/jsm/code/python_backup/python/IoT-botnet/data/UNSW-NB15 - CSV Files/unsw15_train.csv')\n",
    "# temp = raw_data.loc[raw_data['attack_cat'] == 'Normal']\n",
    "# temp_drop = temp.drop(['196', 'attack_cat', 'label'], axis=1, inplace=False)\n",
    "# temp_sameple = temp_drop.sample(1024*60, random_state=Seed)\n",
    "# tr_data = temp_sameple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # batch_size,\n",
    "        # num_workers,\n",
    "        data\n",
    "    ):\n",
    "        # 在数据1维处增加1个维度 example: (batch_size, 196) --> (batch_size, 1, 196)\n",
    "        # self.batch_size = batch_size\n",
    "        # self.num_workers = num_workers\n",
    "        self.data = data.unsqueeze(1)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "dataset = MyDataset(torch.from_numpy(tr_data.values).float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 网络参数初始化\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    # 初始化网络层\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成器\n",
    "class Generator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, in_dim)\n",
    "    Output shape: (N, 1, out_dim)\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, out_dim, dim=32):\n",
    "        super(Generator, self).__init__()\n",
    "        def dconv_bn_relu(in_dim, out_dim):\n",
    "            return nn.Sequential(\n",
    "                nn.ConvTranspose1d(in_dim, out_dim, 5, 2, padding=2, output_padding=1, bias=False),\n",
    "                nn.BatchNorm1d(out_dim),\n",
    "                nn.ReLU()\n",
    "            )\n",
    "        self.inlayer = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim*4*4*4, bias=False),\n",
    "            # tf 默认为0.3， torch 默认为0.01\n",
    "            nn.BatchNorm1d(dim*4*4*4),\n",
    "            nn.ReLU()\n",
    "            # nn.LeakyReLU(negative_slope=0.2)\n",
    "        )\n",
    "        self.midlayer = nn.Sequential(\n",
    "           dconv_bn_relu(dim*4, dim*2),\n",
    "        #    dconv_bn_relu(dim*2, dim*2),\n",
    "           dconv_bn_relu(dim*2, dim),\n",
    "           dconv_bn_relu(dim, 1)\n",
    "        )\n",
    "        self.outlayer = nn.Sequential(\n",
    "            nn.Linear(128, out_dim, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.apply(weights_init)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.inlayer(x)\n",
    "        y = y.view(y.size(0), -1, 16)\n",
    "        y = self.midlayer(y)\n",
    "        y = y.squeeze(1)\n",
    "        y = self.outlayer(y)\n",
    "        y = y.unsqueeze(1)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 判别器\n",
    "class Discriminator(nn.Module):\n",
    "    \"\"\"\n",
    "    Input shape: (N, 1, in_dim)\n",
    "    Output shape: (N, )\n",
    "    \"\"\"\n",
    "    def __init__(self, in_dim, dim=256, in_channel=1, channel=8):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        def conv_bn_lrelu(in_channel, out_channel):\n",
    "            return nn.Sequential(\n",
    "                nn.Conv1d(in_channel, out_channel, 5, 2, 2),\n",
    "                nn.BatchNorm1d(out_channel),\n",
    "                nn.LeakyReLU(0.2),\n",
    "            )\n",
    "\n",
    "        self.inlayer = nn.Sequential(\n",
    "            nn.Linear(in_dim, dim, bias=False),\n",
    "            nn.BatchNorm1d(dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        self.ls = nn.Sequential(\n",
    "            nn.Conv1d(in_channel, channel, 5, 2, 2),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            conv_bn_lrelu(channel, channel * 2),\n",
    "            conv_bn_lrelu(channel * 2, channel * 4),\n",
    "            conv_bn_lrelu(channel * 4, channel * 8),\n",
    "            conv_bn_lrelu(channel * 8, channel * 16),\n",
    "            conv_bn_lrelu(channel * 16, channel * 32),\n",
    "            nn.Conv1d(channel * 32, 1, 4),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        y = x.squeeze(1)\n",
    "        y = self.inlayer(y)\n",
    "        y = y.unsqueeze(1)\n",
    "        y = self.ls(y)\n",
    "        y = y.view(-1)\n",
    "        return y           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int = 16,\n",
    "        out_dim: int = 128,\n",
    "        in_channels: int = 1,\n",
    "        lr: float = 1e-4,\n",
    "        n_critic: int = 5,\n",
    "        clip_value: float = 0.01,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        self.generator = Generator(in_dim=self.hparams.in_dim, out_dim=self.hparams.out_dim)\n",
    "        self.discriminator = Discriminator(in_dim = self.hparams.out_dim)\n",
    "        # self.validation_z = torch.randn(10, self.hparams.in_channels, self.hparams.in_dim)\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def adversarial_loss(self, y_hat, y):\n",
    "        return -torch.mean(self.discriminator(y)) + torch.mean(self.discriminator(y_hat))\n",
    "    \n",
    "    def training_step(self, batch, batch_idx, optimizer_idx):\n",
    "\n",
    "        data = batch\n",
    "        z = torch.randn(self.hparams.batch_size, self.hparams.in_dim)\n",
    "        z = z.type_as(data)\n",
    "        # train generator\n",
    "        if optimizer_idx == 0 and (batch_idx % self.hparams.n_critic == 0 and batch_idx != 0):\n",
    "            # print('batch_idx {}, optimizer_idx{}'.format(batch_idx, optimizer_idx))\n",
    "            # generate data\n",
    "            self.generated_data = self(z)\n",
    "\n",
    "            # generator of WGAN loss\n",
    "            g_loss = -torch.mean(self.discriminator(self(z)))\n",
    "            self.logger.experiment.add_scalar(\"g_loss\", g_loss.detach(), self.current_epoch)\n",
    "            tqdm_dict = {\"g_loss\": g_loss}\n",
    "            output = OrderedDict({\"loss\": g_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n",
    "            return output\n",
    "\n",
    "        # train discriminator\n",
    "        if optimizer_idx == 1:\n",
    "            \n",
    "            # print('batch_idx {}, optimizer_idx{}'.format(batch_idx, optimizer_idx))\n",
    "            # discriminator of WGAN loss\n",
    "            d_loss = -torch.mean(self.discriminator(data)) + torch.mean(self.discriminator(self(z)))\n",
    "            self.logger.experiment.add_scalar(\"d_loss\", d_loss.detach(), self.current_epoch)\n",
    "            # Clip weights of discriminator\n",
    "            for p in self.discriminator.parameters():\n",
    "                p.data.clamp_(-self.hparams.clip_value, self.hparams.clip_value)\n",
    "\n",
    "            tqdm_dict = {\"d_loss\": d_loss}\n",
    "            output = OrderedDict({\"loss\": d_loss, \"progress_bar\": tqdm_dict, \"log\": tqdm_dict})\n",
    "            return output\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "\n",
    "        opt_g = torch.optim.RMSprop(self.generator.parameters(), lr=lr)\n",
    "        opt_d = torch.optim.RMSprop(self.discriminator.parameters(), lr=lr)\n",
    "        \n",
    "        return [opt_g, opt_d], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_dataloder = DataLoader(dataset, batch_size=BATCH_SIZE, num_workers=NUM_WORKERS)\n",
    "# in_dim = tr_data.shape[-1]\n",
    "in_dim = 16\n",
    "out_dim = tr_data.shape[-1]\n",
    "wgan = WGAN(in_dim, out_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsm/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/callback_connector.py:90: LightningDeprecationWarning: Setting `Trainer(progress_bar_refresh_rate=20)` is deprecated in v1.5 and will be removed in v1.7. Please pass `pytorch_lightning.callbacks.progress.TQDMProgressBar` with `refresh_rate` directly to the Trainer's `callbacks` argument instead. Or, to disable the progress bar pass `enable_progress_bar = False` to the Trainer.\n",
      "  rank_zero_deprecation(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 113 K \n",
      "1 | discriminator | Discriminator | 271 K \n",
      "------------------------------------------------\n",
      "384 K     Trainable params\n",
      "0         Non-trainable params\n",
      "384 K     Total params\n",
      "1.540     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/240 [00:00<?, ?it/s] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jsm/miniconda3/envs/pytorch/lib/python3.8/site-packages/pytorch_lightning/loops/optimization/closure.py:35: LightningDeprecationWarning: One of the returned values {'log', 'progress_bar'} has a `grad_fn`. We will detach it automatically but this behaviour will change in v1.6. Please detach it manually: `return {'loss': ..., 'something': something.detach()}`\n",
      "  rank_zero_deprecation(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 240/240 [00:03<00:00, 74.66it/s, loss=-0.00305, v_num=3]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    gpus = AVAIL_GPUS,\n",
    "    max_epochs=100,\n",
    "    progress_bar_refresh_rate = 20\n",
    ")\n",
    "trainer.fit(wgan, tr_dataloder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de5b82254768225213474ab4669cea3d52fe6b864ad6c9d79489ae1089fd4498"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
