{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "# network\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from torchmetrics import F1\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import gc\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    " # shuffle 是否将官方给的的测试集和训练集重新打乱，再分成新的的训练集和测试集\n",
    " # ss标准化\n",
    "def process_data(tr_data, te_data, ss=None, shuffle=False):\n",
    "    split_num = len(tr_data)\n",
    "    data_temp = pd.concat([tr_data, te_data], axis=0)\n",
    "    data = pd.get_dummies(data_temp.iloc[:, 1:-2])\n",
    "    data['cat_code'] = LabelEncoder().fit_transform(data_temp.loc[:, 'attack_cat'])\n",
    "    # data['label'] = data_temp['label']\n",
    "    # data['attack_cat'] = data_temp['attack_cat']\n",
    "    if ss is None:\n",
    "        data.iloc[:,:-3] = ss.fit_transform(data.iloc[:,:-3])\n",
    "    if shuffle:\n",
    "        pass\n",
    "    else:\n",
    "        return data.iloc[:split_num,:], data.iloc[split_num:, :]\n",
    "\n",
    "ss = StandardScaler()\n",
    "tr_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_testing-set.csv')\n",
    "te_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_training-set.csv')\n",
    "tr_data, te_data = process_data(tr_raw_data, te_raw_data, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(np.int64)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tr_data,\n",
    "        te_data,\n",
    "        val_num: float = 0.1,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        num_workers: int = NUM_WORKERS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    # def prepare_data(self):\n",
    "        self.tr_data = MyDataset(tr_data[:,:-1], tr_data[:,-1])\n",
    "        self.test_set = MyDataset(te_data[:,:-1], te_data[:,-1])\n",
    "    def setup(self, stage = None):\n",
    "        # 划分训练集、验证集、测试集\n",
    "        if stage in (None, \"fit\"):\n",
    "            total_num = len(self.tr_data)\n",
    "            val_num = int (total_num * 0.1)\n",
    "            self.train_set, self.val_set = random_split(self.tr_data, [total_num-val_num, val_num])\n",
    "            del self.tr_data\n",
    "            gc.collect()\n",
    "        # if stage in (None, \"test\"):\n",
    "        #     self.te_data\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_class,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout: float = 0.5,\n",
    "        lr: float = 0.001,\n",
    "        b1: float = 0.9,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_class),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        num_layers = self.hparams.num_layers\n",
    "        batch_size = self.hparams.batch_size\n",
    "        hidden_dim = self.hparams.hidden_dim\n",
    "        # h0 = torch.randn(num_layers, batch_size, hidden_dim)\n",
    "        # c0 = torch.randn(num_layers, batch_size, hidden_dim)\n",
    "        # x, _ = self.lstm(inputs, (h0, c0))\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def evaluation(self, y_pred, y_true):\n",
    "        # if statue == 'test':\n",
    "        # 待写 判断调用时模型的过程（），\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        # # 计算每个类别的F1值，最后求平均\n",
    "        # precision = precision_score(y_true, y_pred, average='macro')\n",
    "        # f1score = avg_f1(y_true, y_pred, average='macro')\n",
    "        # recall = recall_score(y_true, y_pred, average='macro')\n",
    "\n",
    "        #  通过先计算总体的TP，FN和FP的数量，再计算F1\n",
    "        precision = precision_score(y_true, y_pred, average='micro')\n",
    "        f1score = f1_score(y_true, y_pred, average='micro')\n",
    "        recall = recall_score(y_true, y_pred, average='micro')\n",
    "        metrics = {\n",
    "            # 'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1score,\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_true = y.cpu().detach().numpy()\n",
    "        y_pred = torch.argmax(y_hat, dim=1).cpu().numpy()\n",
    "        metrics = self.evaluation(y_true, y_pred)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        metrics['cat_loss'] = loss.cpu().numpy()\n",
    "        # self.log_dict(metrics, prog_bar=True, on_epoch=True)\n",
    "        return metrics\n",
    "\n",
    "    def validation_epoch_end(self, outputs):\n",
    "        # outputs = outputs.cpu().numpy()\n",
    "        avg_loss = np.stack([x['cat_loss'] for x in outputs]).mean()\n",
    "        avg_accuracy = np.stack([x['accuracy'] for x in outputs]).mean()\n",
    "        avg_precision = np.stack([x['precision'] for x in outputs]).mean()\n",
    "        avg_recall = np.stack([x['recall'] for x in outputs]).mean()\n",
    "        avg_f1 = np.stack([x['f1_score'] for x in outputs]).mean()\n",
    "        avg_metrics = {\n",
    "            'avg_loss': avg_loss,\n",
    "            'avg_accuracy': avg_accuracy,\n",
    "            'avg_precision': avg_precision,\n",
    "            'avg_recall': avg_recall,\n",
    "            'avg_f1': avg_f1,\n",
    "        }\n",
    "        # return metrics\n",
    "        self.log_dict(avg_metrics, prog_bar=True)\n",
    "        return avg_metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_true = y.cpu().detach().numpy()\n",
    "        y_pred = torch.argmax(y_hat, dim=1).cpu().numpy()\n",
    "        metrics = self.evaluation(y_true, y_pred)\n",
    "        # self.print('metrics:', metrics)\n",
    "        self.log_dict(metrics, prog_bar=True, on_epoch=True)\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        return torch.optim.Adam(self.parameters(), lr = lr, betas = (b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name       | Type       | Params\n",
      "------------------------------------------\n",
      "0 | lstm       | LSTM       | 299 K \n",
      "1 | classifier | Sequential | 1.3 K \n",
      "------------------------------------------\n",
      "300 K     Trainable params\n",
      "0         Non-trainable params\n",
      "300 K     Total params\n",
      "1.201     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38: 100%|██████████| 1370/1370 [00:16<00:00, 81.91it/s, loss=1.82, v_num=3, avg_loss=1.830, avg_accuracy=0.528, avg_precision=0.528, avg_recall=0.528, avg_f1=0.528]\n"
     ]
    }
   ],
   "source": [
    "seed_everything(42)\n",
    "dm = MyDataModule(tr_data.values, te_data.values)\n",
    "model = MyLSTM(10, 196, 128, 2)\n",
    "early_stop_callback = EarlyStopping(monitor=\"avg_f1\", min_delta=0.00, patience=10, verbose=False, mode=\"max\")\n",
    "trainer = Trainer(gpus = AVAIL_GPUS, max_epochs=100, callbacks=[early_stop_callback])\n",
    "trainer.fit(model, dm)\n",
    "# trainer.validate(model, dm, callbacks=[EarlyStopping(monitor=\"avg_loss\")])\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing:  99%|█████████▊| 635/644 [00:03<00:00, 213.05it/s]--------------------------------------------------------------------------------\n",
      "DATALOADER:0 TEST RESULTS\n",
      "{'accuracy': 0.5837705731391907,\n",
      " 'f1_score': 0.5837705731391907,\n",
      " 'precision': 0.5837705731391907,\n",
      " 'recall': 0.5837705731391907}\n",
      "--------------------------------------------------------------------------------\n",
      "Testing: 100%|██████████| 644/644 [00:03<00:00, 194.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'accuracy': 0.5837705731391907,\n",
       "  'precision': 0.5837705731391907,\n",
       "  'recall': 0.5837705731391907,\n",
       "  'f1_score': 0.5837705731391907}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.test(model, dm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de5b82254768225213474ab4669cea3d52fe6b864ad6c9d79489ae1089fd4498"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
