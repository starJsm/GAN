{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data process\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# metrics\n",
    "from sklearn.metrics import roc_auc_score, recall_score, precision_score, f1_score, accuracy_score\n",
    "\n",
    "# network\n",
    "import torch\n",
    "from pytorch_lightning import LightningDataModule, LightningModule, Trainer, seed_everything\n",
    "# from torchmetrics import F1\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "import gc\n",
    "\n",
    "AVAIL_GPUS = min(1, torch.cuda.device_count())\n",
    "BATCH_SIZE = 128\n",
    "NUM_WORKERS = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " # shuffle 是否将官方给的的测试集和训练集重新打乱，再分成新的的训练集和测试集\n",
    " # ss标准化\n",
    "def process_data(tr_data, te_data, ss=None, shuffle=False):\n",
    "    split_num = len(tr_data)\n",
    "    data_temp = pd.concat([tr_data, te_data], axis=0)\n",
    "    data = pd.get_dummies(data_temp.iloc[:, 1:-2])\n",
    "    data['cat_code'] = LabelEncoder().fit_transform(data_temp.loc[:, 'attack_cat'])\n",
    "    # data['label'] = data_temp['label']\n",
    "    # data['attack_cat'] = data_temp['attack_cat']\n",
    "    if ss is None:\n",
    "        data.iloc[:,:-3] = ss.fit_transform(data.iloc[:,:-3])\n",
    "    if shuffle:\n",
    "        pass\n",
    "    else:\n",
    "        return data.iloc[:split_num,:], data.iloc[split_num:, :]\n",
    "\n",
    "ss = StandardScaler()\n",
    "tr_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_testing-set.csv')\n",
    "te_raw_data = pd.read_csv('/home/jsm/code/python/unsupervisedGAN/data/UNSW-NB15/part/UNSW_NB15_training-set.csv')\n",
    "tr_data, te_data = process_data(tr_raw_data, te_raw_data, ss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.data = torch.from_numpy(X).float()\n",
    "        if y is not None:\n",
    "            y = y.astype(np.int)\n",
    "            self.label = torch.LongTensor(y)\n",
    "        else:\n",
    "            self.label = None\n",
    "    def __getitem__(self, idx):\n",
    "        if self.label is not None:\n",
    "            return self.data[idx], self.label[idx]\n",
    "        else:\n",
    "            return self.data[index]\n",
    "    def __len__(self):\n",
    "        return len(self.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataModule(LightningDataModule):\n",
    "    def __init__(\n",
    "        self, \n",
    "        tr_data,\n",
    "        te_data,\n",
    "        val_num: float = 0.1,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        num_workers: int = NUM_WORKERS,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "\n",
    "    # def prepare_data(self):\n",
    "        self.tr_data = MyDataset(tr_data[:,:-1], tr_data[:,-1])\n",
    "        self.test_set = MyDataset(te_data[:,:-1], te_data[:,-1])\n",
    "    def setup(self, stage = None):\n",
    "        # 划分训练集、验证集、测试集\n",
    "        if stage in (None, \"fit\"):\n",
    "            total_num = len(self.tr_data)\n",
    "            val_num = int (total_num * 0.1)\n",
    "            self.train_set, self.val_set = random_split(self.tr_data, [total_num-val_num, val_num])\n",
    "            del self.tr_data\n",
    "            gc.collect()\n",
    "        # if stage in (None, \"test\"):\n",
    "        #     self.te_data\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_set,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_set,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.test_set,\n",
    "            batch_size = self.batch_size,\n",
    "            num_workers = self.num_workers,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLSTM(LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_class,\n",
    "        input_dim,\n",
    "        hidden_dim,\n",
    "        num_layers,\n",
    "        dropout: float = 0.5,\n",
    "        lr: float = 0.001,\n",
    "        b1: float = 0.9,\n",
    "        b2: float = 0.999,\n",
    "        batch_size: int = BATCH_SIZE,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # networks\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, num_class),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        num_layers = self.hparams.num_layers\n",
    "        batch_size = self.hparams.batch_size\n",
    "        hidden_dim = self.hparams.hidden_dim\n",
    "        # h0 = torch.randn(num_layers, batch_size, hidden_dim)\n",
    "        # c0 = torch.randn(num_layers, batch_size, hidden_dim)\n",
    "        # x, _ = self.lstm(inputs, (h0, c0))\n",
    "        inputs = torch.unsqueeze(inputs, 1)\n",
    "        x, _ = self.lstm(inputs, None)\n",
    "        x = x[:, -1, :]\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "    def evaluation(self, y_pred, y_true):\n",
    "        # if statue == 'test':\n",
    "        # 待写 判断调用时模型的过程（），\n",
    "        accuracy = accuracy_score(y_true, y_pred)\n",
    "        precision = precision_score(y_true, y_pred, average='macro')\n",
    "        f1score = f1_score(y_true, y_pred, average='macro')\n",
    "        recall = recall_score(y_true, y_pred, average='macro')\n",
    "        metrics = {\n",
    "            # 'loss': loss,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1_score': f1score,\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_true = y.cpu().detach().numpy()\n",
    "        y_pred = torch.argmax(y_hat, dim=1).cpu().numpy()\n",
    "        metrics = self.evaluation(y_true, y_pred)\n",
    "        loss = F.cross_entropy(y_hat, y)\n",
    "        metrics['loss'] = loss\n",
    "        self.log_dict(metrics, prog_bar=True, on_epoch=True)\n",
    "        return metrics\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        y_true = y.cpu().detach().numpy()\n",
    "        y_pred = torch.argmax(y_hat, dim=1).cpu().numpy()\n",
    "        metrics = self.evaluation(y_true, y_pred)\n",
    "        # self.print('metrics:', metrics)\n",
    "        self.log_dict(metrics, prog_bar=True, on_epoch=True)\n",
    "        return metrics\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        lr = self.hparams.lr\n",
    "        b1 = self.hparams.b1\n",
    "        b2 = self.hparams.b2\n",
    "        return torch.optim.Adam(self.parameters(), lr = lr, betas = (b1, b2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)\n",
    "dm = MyDataModule(tr_data.values, te_data.values)\n",
    "model = MyLSTM(10, 196, 128, 2)\n",
    "trainer = Trainer(gpus = AVAIL_GPUS, max_epochs=5, progress_bar_refresh_rate=20)\n",
    "trainer.fit(model, dm)\n",
    "trainer.test(model, dm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "de5b82254768225213474ab4669cea3d52fe6b864ad6c9d79489ae1089fd4498"
  },
  "kernelspec": {
   "display_name": "Python 3.7.10 64-bit ('pytorch': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
